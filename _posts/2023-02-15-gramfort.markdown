---
layout: post
title:  "Gramfort's Lowess Approach"
date:   2023-02-15 09:26:54 -0500
categories: projects
permalink: 
usemathjax: true
---

Alex Gramfort created his own approach for applying locally weighted linear regression (source code found [here](https://gist.github.com/agramfort/850437)).  

Let's take a look at his function: 
{% highlight python %}
from math import ceil
import numpy as np
from scipy import linalg


def lowess(x, y, f=2. / 3., iter=3):
    """lowess(x, y, f=2./3., iter=3) -> yest
    Lowess smoother: Robust locally weighted regression.
    The lowess function fits a nonparametric regression curve to a scatterplot.
    The arrays x and y contain an equal number of elements; each pair
    (x[i], y[i]) defines a data point in the scatterplot. The function returns
    the estimated (smooth) values of y.
    The smoothing span is given by f. A larger value for f will result in a
    smoother curve. The number of robustifying iterations is given by iter. The
    function will run faster with a smaller number of iterations.
    """
    n = len(x)
    r = int(ceil(f * n))
    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]
    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)
    w = (1 - w ** 3) ** 3
    yest = np.zeros(n)
    delta = np.ones(n)
    for iteration in range(iter):
        for i in range(n):
            weights = delta * w[:, i]
            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])
            A = np.array([[np.sum(weights), np.sum(weights * x)],
                          [np.sum(weights * x), np.sum(weights * x * x)]])
            beta = linalg.solve(A, b)
            yest[i] = beta[0] + beta[1] * x[i]

        residuals = y - yest
        s = np.median(np.abs(residuals))
        delta = np.clip(residuals / (6.0 * s), -1, 1)
        delta = (1 - delta ** 2) ** 2

    return yest
{% endhighlight %}

### Some key things to notice:

1. The parameter, **f**, essentially defines the width of the 
neighborhood of a given value. 
2. The linear regressions are computed multiple times (according 
to the parameter **iter**), so that for each subsequent round of 
calculations, the values with the highest residuals (i.e. the outliers)
are excluded, allowing the curve to become smoother. 

### A walkthrough of the function

{% highlight python %}
# number of observations
n = len(x)
{% endhighlight %}

Throughout the function, we will be looking at one observation
at a time, and for each one we will:
- Define its neighborhood by finding the points that are closest in 
vicinity (using its Euclidean distance from all other points)
and using f to restrict the neighborhood.  
- Create weights that correspond to that one value.  The 
weights will scale all the other observations, so the dimensions
of the weights for 1 observation is the same as the dimensions of X. 
- Create a linear regression model using the weights from that observation.

$$\;$$
{% highlight python %}
# get index of furthest neighbor
r = int(ceil(f * n))
{% endhighlight %}

Say we leave **f** at its default value, $$\frac{2}{3}$$.  This 
line will get the index at the $$f^{th}$$, or $$\frac{2}{3}^{rd}$$, mark in the data.
For example, if we have $$100$$ observations, then **r** would be
calculated to be $$67$$. 

$$\;$$
{% highlight python %}
# get distance from furthest neighbor for each observation
h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)] # [n]
{% endhighlight %}
For each observation, this line 1) calculates the distance between
the observation and all other observations, 2) gets the $$r^{th}$$
value among these distances (or its distance from its furthest neighbor)
, and puts them all in a list of size n. The distances between
observation[i] and all other observations will later be divided by
 $$h[i]$$, so that values closer to observation[i] will be closer 
 to 0, and values outside of the neighborhood will be 1. 





$$\;$$
{% highlight python %}
w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)
w = (1 - w ** 3) ** 3
{% endhighlight %}
In this line, ```x[:, None] - x[None, :]``` again calculates the pairwise
distances (which is why the absolute value function is applied), and 
each pairwise distance from observation[i] will be divided by its
corresponding distance from its furthest neighbor h.  

Then, all values
are clipped to be between 0 and 1.  This means that
values close to observation[i] will be zero, and all values outside of 
the neighborhood of observation[i] are 1.  This produces a matrix of weights
of size $$[n,n]$$, because you have a vector of size $$n$$ to define the
neighborhood of observation[i], and you have $$n$$ observations requiring 
$$n$$ vectors of weights.  

For multi-dimensional data, these weights 
will have to be matrices, so you will end up with a weights matrix with 
dimensions $$[n,n,p]$$ where $$p$$ is the number of features.

The line ```w = (1 - w ** 3) ** 3``` simply applies the tricubic kernel
function to the weights. In a sense, this 'flips' the numbers so that 0s 
become 1s and vice versa.  i.e. closer to 1 means closer to the central 
observation.


$$\;$$
{% highlight python %}
# initialize vector for predictions
yest = np.zeros(n)
# initialize vector for smoothing curve
delta = np.ones(n)
{% endhighlight %}







$$\;$$
{% highlight python %}
for iteration in range(iter):
{% endhighlight %}
The remainder of the function (before the return statement) is 
nested below this call for iteration. Each time the rest of the 
code runs, there is the potential for the influence of some 
values to be decreased in order to reduce the residuals and
smooth out the curve. 


$$\;$$
{% highlight python %}
for i in range(n):
    weights = delta * w[:, i]
    b = np.array([np.sum(weights * y), np.sum(weights * y * x)])
    A = np.array([[np.sum(weights), np.sum(weights * x)],
                    [np.sum(weights * x), np.sum(weights * x * x)]])
    beta = linalg.solve(A, b)
    yest[i] = beta[0] + beta[1] * x[i]
{% endhighlight %}
For each observation, a linear regression equation is solved.  
First, the weights are reweighted by delta: ```weights = delta * w[:, i]```.
In the first iteration, this line does nothing because all values of 
delta are 1. 

The following three lines apply the weights to the observations and
dependent variables, then solve a system of equations to produce
a linear regression model.  The line ```yest[i] = beta[0] + beta[1] * x[i]```
applies the model to the prediction of the dependent variable 
with obervation$$[i]$$ as input, and adds it to the $$yest$$ vector.


$$\;$$
{% highlight python %}
residuals = y - yest
s = np.median(np.abs(residuals))
delta = np.clip(residuals / (6.0 * s), -1, 1)
delta = (1 - delta ** 2) ** 2
{% endhighlight %}
This is the robustifying/smoothing part of the algorithm.  The median
value of the residuals are calculated.  The line
```np.clip(residuals / (6.0 * s), -1, 1)``` essentially 
makes outliers (with high residuals) have delta values
of -1 or 1, so that the function in the next line flips
the delta values of outliers of zero.  This means that
future iterations of the linear regression model do 
not consider outliers. 


$$\;$$
{% highlight python %}
return yest
{% endhighlight %}
The predictions for all observations are returned. 

## My adaptations

### Goals

1. Accommodate multidimensional features
2. Accommodate train and test sets
3. Create SciKitLearn-compliant version
4. Test with k-Fold cross-validations and GridSearchCV

### Conceptual changes

To allow multidimensional features, any calculation of the distances 
in the code which simply subtracts features and take the absolute
value will have to be replaced with the calculation of Euclidean distance. 

Assuming that the data is scaled, weights will now be multiplied to 
observation vectors of multiple values.  

To solve the linear regression equation, it will be easier to use 
SciKitLearn's LinearRegression function which works with multi-dimensional
data.  This will make it easier to encorporate testing data, as 
it only requires calling the predict method. 

However, since LOWESS is a composite of linear regression models 
for each point, we cannot directly predict new data points, so we 
must use some version of interpolation to do so. 

My modified function is shown below:


### Modified Gramfort's


{% highlight python %}
def lowess_ag(x, y, xnew, f=2. / 3., iter=3):
    n = len(x)
    new = len(xnew) 
    r = int(ceil(f * n))

    # convert 1D data to correct form
    if len(y.shape)==1: 
      y = y.reshape(-1,1)

    if len(x.shape)==1:
      x = x.reshape(-1,1)
      xnew = xnew.reshape(-1,1)

    # generate weights matrix w - notice the use of Euclidean distances
    h = [np.sort(np.sqrt(np.sum((x - x[i])**2,axis=1)))[r] for i in range(n)]
    w = np.clip(np.array([np.sqrt(np.sum((x - x[i])**2, axis=1))/h[i] for i in range(n)]), 0.0, 1.0) 
    w = (1 - w ** 3) ** 3

    # create and robustify model using xtrain,ytrain
    yest = np.zeros(n)
    delta = np.ones(n)
    for iteration in range(iter):
      for i in range(n):
          weights = delta * w[i]
          lm.fit(np.diag(weights).dot(x),np.diag(weights).dot(y.reshape(-1,1)))
          yest[i] = lm.predict(x[i].reshape(1,-1))

      residuals = np.sqrt(np.sum((y - yest)**2))
      s = np.median(np.abs(residuals))
      delta = np.clip(residuals / (6.0 * s), -1, 1)
      delta = (1 - delta ** 2) ** 2

    # predict from new data via interpolation
    if x.shape[1]==1:
      f = interp1d(x.flatten(),yest,fill_value='extrapolate')
      output = f(xnew)
    else:
      output = np.zeros(len(xnew))
      for i in range(len(xnew)):
        # get the smallest r distances from x_new[i] (x_new[i]'s neighborhood)
        ind = np.argsort(np.sqrt(np.sum((x-xnew[i])**2,axis=1)))[:r]

        # ensure that you don't have too many PCs
        if len(xnew.shape) == 1:
          pca = PCA(n_components=1)
        elif min(xnew.shape[0],xnew.shape[1]) < 3: 
          pca = PCA(n_components=min(xnew.shape[0],xnew.shape[1]))
        else: 
          pca = PCA(n_components=3)
        
        # interpolate from PCs
        x_pca = pca.fit_transform(x[ind])
        tri = Delaunay(x_pca,qhull_options='QJ')
        f = LinearNDInterpolator(tri,y[ind])
        output[i] = f(pca.transform(xnew[i].reshape(1,-1))) 
    # take care of null values
    if sum(np.isnan(output))>0:
      g = NearestNDInterpolator(x,y.ravel()) 
      output[np.isnan(output)] = g(xnew[np.isnan(output)])
    return output

{% endhighlight %}

### Conversion to SkiKitLearn Compliant Function

{% highlight python %}
class Lowess:
    def __init__(self, f=2. / 3., iter=3):
        self.f = f
        self.iter = iter
    
    def fit(self, x, y):
        self.xtrain_ = x
        self.yhat_ = y

    def predict(self, xnew):
        check_is_fitted(self)
        x = self.xtrain_
        y = self.yhat_
        return lowess_ag(x,y,xnew, self.f, self.iter)
{% endhighlight %}


### KFold Cross Validations

I am using a cars dataset with 392 observations to perform testing on the function. 
I will model the miles per gallon of a given car using either the weight as a predictor
or the weight and engine details.  

![](../assets/images/cars_data.png)


Modeling miles per gallon only according to weight gave the following model using
the default hyperparameters (with a random forest model for comparison):

![](../assets/images/cars_plot.png)

![]({{site.baseurl}}/assets/images/cars_plot.png)


Here is the code for my KFold Cross validation setup:

{% highlight python %}
x = np.array(data.loc[:,'CYL':'WGT'])
y = np.array(data.loc[:,'MPG'])

mse_lwr = []
mse_rf = []
kf = KFold(n_splits=10,shuffle=True,random_state=1234)
model_rf = RandomForestRegressor(n_estimators=200,max_depth=5)
model_lw = Lowess(f=1/65,iter=3)

for idxtrain, idxtest in kf.split(x):
  xtrain = x[idxtrain]
  ytrain = y[idxtrain]
  ytest = y[idxtest]
  xtest = x[idxtest]
  xtrain = scale.fit_transform(xtrain)
  xtest = scale.transform(xtest)

  model_lw.fit(xtrain,ytrain)
  yhat_lw = model_lw.predict(xtest)
  
  model_rf.fit(xtrain,ytrain)
  yhat_rf = model_rf.predict(xtest)

  mse_lwr.append(mse(ytest,yhat_lw))
  mse_rf.append(mse(ytest,yhat_rf))
print('The Cross-validated Mean Squared Error for Locally Weighted Regression is : '+str(np.mean(mse_lwr)))
print('The Cross-validated Mean Squared Error for Random Forest is : '+str(np.mean(mse_rf)))
{% endhighlight %}

Results: 
```
The Cross-validated Mean Squared Error for Locally Weighted Regression is : 24.508625902360556
The Cross-validated Mean Squared Error for Random Forest is : 17.15486212027211
```

As you can see, our model does a little worse than the Random Forest model, but 
we can try to tune the model's hyperparameters to produce a better MSE. 

### Test with GridSearchCV

I performed the following GridSearchCV test for optimal hyperparameters:

{% highlight python %}
X = np.array(data.loc[:300,'ENG':'WGT'])
y = np.array(data.loc[:300,'MPG'])

lw = Lowess()
param_grid = dict(f=[1/40, 1/30, 1/20,1/4,1/3,1/2,2/3],iter = [1,2,3,4]) 
cv = KFold(n_splits = 10, random_state = 146, shuffle = True) 
grid = GridSearchCV(lw, param_grid = param_grid, cv = cv, scoring = 'r2')

grid.fit(X,y)
{% endhighlight %}

It returned that:
```
The best parameters are {'f': 0.03333333333333333, 'iter': 1} with a score of 0.80
```

Rerunning the KFold Cross-validations produced the following MSE:
```
The Cross-validated Mean Squared Error for Locally Weighted Regression is : 22.226274854534292
```