---
layout: post
title:  "Gramfort's Lowess Approach"
date:   2023-02-15 09:26:54 -0500
categories: projects
permalink: 
usemathjax: true
---

Alex Gramfort created his own approach for applying locally weighted linear regression (source code found [here](https://gist.github.com/agramfort/850437)).  

Let's take a look at his function: 
{% highlight ruby %}
from math import ceil
import numpy as np
from scipy import linalg


def lowess(x, y, f=2. / 3., iter=3):
    """lowess(x, y, f=2./3., iter=3) -> yest
    Lowess smoother: Robust locally weighted regression.
    The lowess function fits a nonparametric regression curve to a scatterplot.
    The arrays x and y contain an equal number of elements; each pair
    (x[i], y[i]) defines a data point in the scatterplot. The function returns
    the estimated (smooth) values of y.
    The smoothing span is given by f. A larger value for f will result in a
    smoother curve. The number of robustifying iterations is given by iter. The
    function will run faster with a smaller number of iterations.
    """
    n = len(x)
    r = int(ceil(f * n))
    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]
    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)
    w = (1 - w ** 3) ** 3
    yest = np.zeros(n)
    delta = np.ones(n)
    for iteration in range(iter):
        for i in range(n):
            weights = delta * w[:, i]
            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])
            A = np.array([[np.sum(weights), np.sum(weights * x)],
                          [np.sum(weights * x), np.sum(weights * x * x)]])
            beta = linalg.solve(A, b)
            yest[i] = beta[0] + beta[1] * x[i]

        residuals = y - yest
        s = np.median(np.abs(residuals))
        delta = np.clip(residuals / (6.0 * s), -1, 1)
        delta = (1 - delta ** 2) ** 2

    return yest
{% endhighlight %}

### Some key things to notice:

1. The parameter, **f**, essentially defines the width of the 
neighborhood of a given value. 
2. The linear regressions are computed multiple times (according 
to the parameter **iter**), so that for each subsequent round of 
calculations, the values with the highest residuals (i.e. the outliers)
are excluded, allowing the curve to become smoother. 

### A walkthrough of the function

{% highlight ruby %}
# number of observations
n = len(x)
{% endhighlight %}

Throughout the function, we will be looking at one observation
at a time, and for each one we will:
- Define its neighborhood by finding the points that are closest in 
vicinity (using its Euclidean distance from all other points)
and using f to restrict the neighborhood.  
- Create weights that correspond to that one value.  The 
weights will scale all the other observations, so the dimensions
of the weights for 1 observation is the same as the dimensions of X. 
- Create a linear regression model using the weights from that observation.

$$\;$$
{% highlight ruby %}
# get index of furthest neighbor
r = int(ceil(f * n))
{% endhighlight %}

Say we leave **f** at its default value, $$\frac{2}{3}$$.  This 
line will get the index at the $$f^{th}$$, or $$\frac{2}{3}^{rd}$$, mark in the data.
For example, if we have $$100$$ observations, then **r** would be
calculated to be $$67$$. 

$$\;$$
{% highlight ruby %}
# get distance from furthest neighbor for each observation
h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)] # [n]
{% endhighlight %}
For each observation, this line 1) calculates the distance between
the observation and all other observations, 2) gets the $$r^{th}$$
value among these distances (or its distance from its furthest neighbor)
, and puts them all in a list of size n. The distances between
observation[i] and all other observations will later be divided by
 $$h[i]$$, so that values closer to observation[i] will be closer 
 to 0, and values outside of the neighborhood will be 1. 





$$\;$$
{% highlight ruby %}
w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)
w = (1 - w ** 3) ** 3
{% endhighlight %}
In this line, ```x[:, None] - x[None, :]``` again calculates the pairwise
distances (which is why the absolute value function is applied), and 
each pairwise distance from observation[i] will be divided by its
corresponding distance from its furthest neighbor h.  

Then, all values
are clipped to be between 0 and 1.  This means that
values close to observation[i] will be zero, and all values outside of 
the neighborhood of observation[i] are 1.  This produces a matrix of weights
of size $$[n,n]$$, because you have a vector of size $$n$$ to define the
neighborhood of observation[i], and you have $$n$$ observations requiring 
$$n$$ vectors of weights.  

For multi-dimensional data, these weights 
will have to be matrices, so you will end up with a weights matrix with 
dimensions $$[n,n,p]$$ where $$p$$ is the number of features.

The line ```w = (1 - w ** 3) ** 3``` simply applies the tricubic kernel
function to the weights. In a sense, this 'flips' the numbers so that 0s 
become 1s and vice versa.  i.e. closer to 1 means closer to the central 
observation.


$$\;$$
{% highlight ruby %}
# initialize vector for predictions
yest = np.zeros(n)
# initialize vector for smoothing curve
delta = np.ones(n)
{% endhighlight %}







$$\;$$
{% highlight ruby %}
for iteration in range(iter):
{% endhighlight %}
The remainder of the function (before the return statement) is 
nested below this call for iteration. Each time the rest of the 
code runs, there is the potential for the influence of some 
values to be decreased in order to reduce the residuals and
smooth out the curve. 


$$\;$$
{% highlight ruby %}
for i in range(n):
    weights = delta * w[:, i]
    b = np.array([np.sum(weights * y), np.sum(weights * y * x)])
    A = np.array([[np.sum(weights), np.sum(weights * x)],
                    [np.sum(weights * x), np.sum(weights * x * x)]])
    beta = linalg.solve(A, b)
    yest[i] = beta[0] + beta[1] * x[i]
{% endhighlight %}
For each observation, a linear regression equation is solved.  
First, the weights are reweighted by delta: ```weights = delta * w[:, i]```.
In the first iteration, this line does nothing because all values of 
delta are 1. 

The following three lines apply the weights to the observations and
dependent variables, then solve a system of equations to produce
a linear regression model.  The line ```yest[i] = beta[0] + beta[1] * x[i]```
applies the model to the prediction of the dependent variable 
with obervation$$[i]$$ as input, and adds it to the $$yest$$ vector.


$$\;$$
{% highlight ruby %}
residuals = y - yest
s = np.median(np.abs(residuals))
delta = np.clip(residuals / (6.0 * s), -1, 1)
delta = (1 - delta ** 2) ** 2
{% endhighlight %}
This is the robustifying/smoothing part of the algorithm.  The median
value of the residuals are calculated.  The line
```np.clip(residuals / (6.0 * s), -1, 1)``` essentially 
makes outliers (with high residuals) have delta values
of -1 or 1, so that the function in the next line flips
the delta values of outliers of zero.  This means that
future iterations of the linear regression model do 
not consider outliers. 


$$\;$$
{% highlight ruby %}
return yest
{% endhighlight %}
The predictions for all observations are returned. 

## My adaptations

### Goals

1. Accommodate multidimensional features
2. Accommodate train and test sets
3. Create SciKitLearn-compliant version
4. Test with k-Fold cross-validations and GridSearchCV

### Conceptual changes

To allow multidimensional features, any calculation of the distances 
in the code which simply subtracts features and take the absolute
value will have to be replaced with the calculation of Euclidean distance. 

Assuming that the data is scaled, weights will now be multiplied to 
observation vectors of multiple values.  

To solve the linear regression equation, it will be easier to use 
SciKitLearn's LinearRegression function which works with multi-dimensional
data.  This will make it easier to encorporate testing data, as 
it only requires calling the predict method. 